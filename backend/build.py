import os
import google.generativeai as genai

genai.configure(api_key=os.environ["GEMINI_API_KEY"])

# Create the model
generation_config = {
    "temperature": 1,
    "top_p": 0.95,
    "top_k": 40,
    "max_output_tokens": 8192,
    "response_mime_type": "text/plain",
}

chat_session = genai.GenerativeModel(
    model_name="gemini-2.0-flash-exp",
    generation_config=generation_config,
).start_chat(
    history=[
        {
            "role": "user",
            "parts": [
                "Build a rag with python chroma and react OpenAI APIAnd restful servers throughout. Assistant on front end and Python flask server. Store sucessful tasks in vector store chroma db",
            ],
        },
        {
            "role": "model",
            "parts": [
                "Okay, let's break down this ambitious project into manageable parts and outline how we'd build it. This involves a lot of moving pieces, so I'll focus on the core architecture and concepts, providing code snippets and explanations along the way.\n\n**Project Goal:**\n\nA React front-end application that allows users to interact with an AI assistant (powered by OpenAI). The assistant uses Retrieval Augmented Generation (RAG) to provide context-aware responses based on data stored in a Chroma vector database. Successful tasks/interactions are stored back into the Chroma database to enhance future responses. We'll have a Python Flask server acting as the intermediary between the front-end and the OpenAI API and the Chroma DB.\n\n**Core Technologies:**\n\n*   **Frontend:** React (for UI)\n*   **Backend:** Python (Flask for RESTful API)\n*   **AI/LLM:** OpenAI API (for language model capabilities)\n*   **Vector Database:** Chroma DB (for storing and querying embeddings)\n\n**Architecture:**\n\n```\n+-----------------+    (1)  +--------------+    (2)  +-------------------+    (3)   +--------------------+\n|   React Frontend | -------> | Flask Server | -------> | OpenAI API / RAG  | -------> |  Chroma DB          |\n+-----------------+         +--------------+         +-------------------+         +--------------------+\n       ^                               |                                          ^\n       |                               |  (4)                                    | (5)\n       +------------------------------+                                          +-----------------------+\n             (User interaction/response)                          (Successful task embeddings)\n```\n\n1.  **User Input:** The React app captures user input and sends it to the Flask server.\n2.  **API Request & RAG:** The Flask server receives the user input, performs RAG (Retrieval Augmented Generation) using Chroma DB and the OpenAI API, and returns a response.\n3.  **OpenAI API Interaction:** The Flask server uses the OpenAI API to generate a response based on the retrieved context.\n4.  **User Output:** The React app receives the response from the Flask server and displays it to the user.\n5.  **Feedback Loop:** If the interaction is deemed successful (e.g., user confirms the response was helpful), the Flask server stores the user input and the AI response (as embeddings) in the Chroma DB for future context.\n\n**Let's start with the backend (Python Flask server):**\n\n**1. Setting up the Flask Server (app.py)**\n\n```python\nfrom flask import Flask, request, jsonify\nfrom flask_cors import CORS\nimport openai\nimport chromadb\nfrom chromadb.utils import embedding_functions\nimport os\nfrom dotenv import load_dotenv\nimport json\nload_dotenv()\n\napp = Flask(__name__)\nCORS(app)  # Enable CORS for React frontend\n\n# OpenAI API Key\nopenai.api_key = os.getenv(\"OPENAI_API_KEY\")\n\n\n# Set up Chroma DB and Embedding function\npersist_directory = \"chroma_db\"  # path for where to save db\n# Use OpenAIEmbeddings as our function to vectorize data\n#  this vectorizes your data so its searchable with semantic meaning \n#  not just string matching\nopenai_ef = embedding_functions.OpenAIEmbeddingFunction(\n    api_key=openai.api_key, model_name=\"text-embedding-ada-002\"\n)\n\n# create the chroma db client\nclient = chromadb.PersistentClient(path=persist_directory)\ncollection = client.get_or_create_collection(name=\"rag_collection\", embedding_function=openai_ef)\n\ndef get_rag_response(query):\n    \"\"\"Performs RAG (Retrieval Augmented Generation) using Chroma DB and OpenAI API.\"\"\"\n\n    results = collection.query(query_texts=[query], n_results=5)\n    print(results)\n    context = \"\\n\".join([doc for doc in results[\"documents\"][0]])\n    print(\"here is the context\", context)\n    \n    if context:\n        prompt = f\"\"\"\n            Use the following context to answer the question at the end. \n            If you don't know the answer, just say that you don't know, don't try to make up an answer.\n            \n            Context:\n            {context}\n            \n            Question: {query}\n        \"\"\"\n    else:\n          prompt = f\"\"\"\n            You are a helpful assistant\n            Answer: {query}\n        \"\"\"\n    \n    try:\n          completion = openai.ChatCompletion.create(\n            model=\"gpt-3.5-turbo\",\n            messages=[\n            {\"role\": \"user\", \"content\": prompt}\n            ]\n          )\n\n          return completion.choices[0].message.content\n    except Exception as e:\n        print(\"error with open ai\", e)\n        return \"Error with open ai api\"\n    \n@app.route(\"/api/chat\", methods=[\"POST\"])\ndef chat():\n    data = request.get_json()\n    if \"message\" not in data:\n       return jsonify({\"message\": \"Invalid input\"}), 400\n    query = data[\"message\"]\n    \n    # Handle RAG logic using vector database\n    response = get_rag_response(query)\n\n    # store interaction to database for learning\n    if response != \"Error with open ai api\":\n        add_to_chroma_db(query, response)\n\n    return jsonify({\"message\": response})\n\ndef add_to_chroma_db(query, response):\n    \"\"\"Adds the user input and AI response as embeddings to Chroma DB.\"\"\"\n    # add metadata to this with a timestamp to look back on\n    ids = [str(len(collection.get()[\"ids\"])+1)] # make sure ids are unique \n    metadatas = [{\"timestamp\":\"now\"}] \n    \n    # if the database is empty just add everything\n    if len(collection.get()[\"ids\"]) == 0 :\n        collection.add(\n        documents=[query, response],\n        metadatas=metadatas * 2,\n        ids=ids * 2,\n        )\n    else:\n        collection.add(\n        documents=[query, response],\n        metadatas=metadatas * 2,\n        ids=ids * 2,\n        )\n\nif __name__ == \"__main__\":\n    app.run(debug=True)\n```\n\n**Explanation:**\n\n*   **Imports:** Importing necessary libraries for Flask, OpenAI, ChromaDB, and environment variable handling.\n*   **Environment Setup:** Load the OpenAI API key and database persistence directory using dotenv.\n*   **CORS:** Enable CORS to make it accessible for the front end.\n*   **Chroma DB Setup:** Creates a ChromaDB client, embedding function, and a collection to use for data storage.\n*   **`get_rag_response` function:**\n    *   Retrieves top context documents from the Chroma database that matches the query.\n    *   Constructs a prompt for the OpenAI model with context and the original question.\n    *   Sends the prompt to the OpenAI API and returns the response.\n*   **`/api/chat` endpoint:**\n    *   Receives user input via a POST request.\n    *   Calls `get_rag_response` to get the AI-generated response.\n    *   Calls the `add_to_chroma_db` to add to database to learn new info.\n    *   Returns the response as a JSON object.\n*   **`add_to_chroma_db` function:** Adds the user input and the corresponding response to the database.\n\n**2. Environment Variables:**\n   Create a `.env` file in the same directory as your app.py file.\n\n```\nOPENAI_API_KEY=YOUR_OPENAI_API_KEY\n```\n\n**3. Install Packages:**\n   Run this in your terminal:\n   ```\n   pip install flask flask_cors openai chromadb python-dotenv\n   ```\n\n**Now, let's look at the frontend (React):**\n\n**1. Setting up the React Application:**\n\n   If you don't already have a React project set up you can use this method:\n   ```\n   npx create-react-app my-rag-app\n   cd my-rag-app\n   ```\n\n   **2.  Modify `src/App.js`:**\n\n```jsx\nimport React, { useState, useEffect } from \"react\";\nimport \"./App.css\";\n\nfunction App() {\n  const [message, setMessage] = useState(\"\");\n  const [response, setResponse] = useState(\"\");\n  const [chatHistory, setChatHistory] = useState([]);\n\n  const handleInputChange = (e) => {\n    setMessage(e.target.value);\n  };\n\n  const handleSubmit = async (e) => {\n    e.preventDefault();\n    setChatHistory([...chatHistory, { role: \"user\", content: message }]);\n    try {\n      const res = await fetch(\"http://localhost:5000/api/chat\", {\n        method: \"POST\",\n        headers: {\n          \"Content-Type\": \"application/json\",\n        },\n        body: JSON.stringify({ message }),\n      });\n\n      const data = await res.json();\n      setResponse(data.message);\n       setChatHistory([...chatHistory, { role: \"assistant\", content: data.message }]);\n    } catch (error) {\n      console.error(\"Error:\", error);\n      setResponse(\"An error occurred while communicating with the server.\");\n    }\n    setMessage(\"\"); // reset input field\n  };\n\n  return (\n    <div className=\"App\">\n      <h1>RAG Assistant</h1>\n      <div className=\"chat-container\">\n        <div className=\"chat-history\">\n        {chatHistory.map((chat, index) => (\n            <div key={index} className={`chat-message ${chat.role}`}>\n              {chat.content}\n            </div>\n          ))}\n        </div>\n\n        <form onSubmit={handleSubmit}>\n          <input\n            type=\"text\"\n            placeholder=\"Type your message...\"\n            value={message}\n            onChange={handleInputChange}\n          />\n          <button type=\"submit\">Send</button>\n        </form>\n        {response && <div className=\"response\">{response}</div>}\n      </div>\n    </div>\n  );\n}\n\nexport default App;\n```\n\n**3. `src/App.css` for styling:**\n\n```css\n.App {\n  text-align: center;\n  padding: 20px;\n}\n\n.chat-container {\n  display: flex;\n  flex-direction: column;\n  align-items: center;\n  margin-top: 20px;\n}\n.chat-history {\n    max-height: 300px; /* Set a maximum height for the chat history */\n    overflow-y: auto; /* Enable vertical scrolling if content overflows */\n    border: 1px solid #ccc; /* Add a border for visual separation */\n    padding: 10px;\n    width: 80%;\n    margin-bottom: 10px;\n}\n.chat-message {\n  margin-bottom: 5px;\n  padding: 10px;\n  border-radius: 8px;\n  max-width: 80%; /* Adjust as needed */\n}\n\n.chat-message.user {\n  background-color: #e0f7fa;\n  text-align: right;\n  margin-left: auto;\n}\n\n.chat-message.assistant {\n  background-color: #f0f0f0;\n  text-align: left;\n  margin-right: auto;\n}\n\nform {\n  display: flex;\n  gap: 10px;\n  margin-bottom: 10px;\n}\n\ninput {\n  padding: 10px;\n  border: 1px solid #ccc;\n  border-radius: 4px;\n  flex-grow: 1;\n}\n\nbutton {\n  padding: 10px 20px;\n  background-color: #007bff;\n  color: white;\n  border: none;\n  border-radius: 4px;\n  cursor: pointer;\n}\n\n.response {\n  margin-top: 20px;\n  padding: 10px;\n  background-color: #f0f0f0;\n  border: 1px solid #ccc;\n  border-radius: 4px;\n  width: 80%;\n  \n}\n```\n\n**Explanation:**\n\n*   **State Variables:** `message` to store input, `response` for AI response and `chatHistory` to store user and assistant messages.\n*   **Input Handling:** `handleInputChange` updates the message state when the user types.\n*   **API Call:** `handleSubmit` makes a POST request to the Flask server's `/api/chat` endpoint with the user's message.\n*   **Response Handling:** Updates the response state with the data from the API, also appends response and user messages to history\n*   **UI:** Basic input field, submit button, and output area for the AI's response.\n\n**Running the Application:**\n\n1.  **Start Flask server:**\n    ```bash\n    python app.py\n    ```\n\n2.  **Start the React app:**\n    ```bash\n    cd my-rag-app\n    npm start\n    ```\n\n**Key Takeaways:**\n\n*   This is a complex project, and this example gets you started with RAG and saving interactions.\n*   Error handling in both backend and frontend is crucial for production.\n*   Ensure you understand the basics of React, Python, Flask, and the OpenAI API before diving into development.\n*   You can expand this project with more advanced features, data preprocessing, and user experience improvements.\n\nLet me know if you'd like to explore specific aspects or want more detail!\n",
            ],
        },
    ]
)

response = chat_session.send_message("INSERT_INPUT_HERE")

print(response.text)

# To load documents into ChromaDB, run the following command:
# python load_data.py --data_directory <your_data_directory> --persist_directory <your_chroma_db_directory> --collection_name <your_collection_name>