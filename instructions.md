# Gemini RAG Project - README (FINAL BASE)

This document provides a comprehensive guide to setting up, running, and testing the Gemini Retrieval-Augmented Generation (RAG) project. This build serves as the foundational base for all future RAG projects.

---

## Table of Contents

1.  [Project Overview](#project-overview)
2.  [Prerequisites](#prerequisites)
3.  [Backend Setup](#backend-setup)
4.  [Frontend Setup](#frontend-setup)
5.  [Testing](#testing)
6.  [Key Components](#key-components)
7.  [Workflow](#workflow)
8.  [Important Notes](#important-notes)
9.  [Troubleshooting](#troubleshooting)
10. [Future Enhancements](#future-enhancements)

---

## Project Overview

This project demonstrates a Retrieval-Augmented Generation (RAG) system using the Google Gemini API. It allows users to upload documents, query information, and receive answers generated by the LLM, augmented with relevant context from the uploaded documents and the internet. The system features a Concierge Assistant that orchestrates tasks by delegating them to specialized assistants.

---

## Prerequisites

1.  **Operating System:** Windows (tested on Windows 10/11).
2.  **Software Requirements:**
    *   Python 3.8+
    *   Node.js (latest LTS version recommended)
    *   `pip` (Python package installer)
    *   `npm` (Node package manager)
    *   `venv` (Python virtual environment - included with Python 3.3+)
    *   `sentence-transformers` (Python package for relevance scoring)

3.  **API Keys:**
    *   Google Gemini API Key.
    *   Obtain an API key from the Google AI Studio: [https://makersuite.google.com/app/apikey](https://makersuite.google.com/app/apikey)
    *   Create a `.env` file in the `backend` directory with:

        ```properties
        GOOGLE_API_KEY=your_gemini_api_key
        ```

4.  **Vector Database:**
    *   ChromaDB with `PersistentClient`.
    *   Vector data is stored locally in the `./chroma_db` directory within the `backend`. This directory will be created automatically.

---

## Backend Setup

1.  **Navigate to Backend:**

    ```bash
    cd e:\2024 RESET\geminirag\backend
    ```

2.  **Create Virtual Environment:**

    ```bash
    python -m venv venv
    venv\Scripts\activate  # Windows
    # source venv/bin/activate # Linux/macOS
    ```

3.  **Install Dependencies:**

    ```bash
    pip install -r requirements.txt
    ```

4.  **Run Backend:**

    ```bash
    python app.py
    ```

    *   Server starts on `http://localhost:5000`.
    *   Ensure the terminal output shows "Debugger is active!" and "Reloader is active!".

---

## Frontend Setup

1.  **Navigate to Frontend:**

    ```bash
    cd e:\2024 RESET\geminirag\frontend
    ```

2.  **Install Dependencies:**

    ```bash
    npm install
    ```

3.  **Start Frontend:**

    ```bash
    npm start
    ```

    *   App opens in browser at `http://localhost:3000`.

---

## Testing

1.  **Upload Documents:** Use the file upload component in the frontend (`.txt`, `.pdf`, `.docx`, `.csv`).
    *   The uploaded files are stored in the `persistent_documents` directory in the backend.
2.  **Query the Assistant:** Enter a query in the input box and submit. The Concierge Assistant will respond.
    *   For general conversation, just type a greeting or question (e.g., "Hello", "What's the weather like?").
    *   For specific tasks, start with "Can you..." or include "please" (e.g., "Can you summarize this document?", "Please find information about...").
    *   The Concierge will analyze the query and either respond directly or delegate the task to other assistants.
3.  **Inspect Assistant Status:** Observe the progress bars and status messages for each assistant in the header.
4.  **Review Chat History:** Scroll through the chat history to see the interactions and log messages.

---

## Key Components

*   **`App.js` (Frontend):** Main React component, manages state, handles user input, orchestrates tasks, and renders the UI.
*   **`AssistantClass.js` (Frontend):** Defines the `Assistant` class, including the Concierge Assistant's planning and execution logic. Manages individual assistant state and task delegation.
*   **`SkillFunctions.js` (Frontend):** Contains functions that delegate specific tasks to the backend, such as summarizing text, translating text, and performing internet searches.
*   **`app.py` (Backend):** Flask server, handles API requests, interacts with ChromaDB and Gemini API, and manages task routing.
*   **`processing.py` (Backend):** Contains document processing functions (chunking, indexing) and the `perform_internet_search` function.
*   **`gemini/client.py` (Backend):** Handles communication with the Gemini API, including prompt formatting and response parsing.

---

## Workflow

1.  **User Input:** User enters a query in the frontend (`App.js`).
2.  **Concierge Analysis:** `App.js` sends the query to the Concierge Assistant (`AssistantClass.js`).
3.  **Task Determination:** The Concierge determines if the query is a conversation or a task request.
    *   **Conversation:** The Concierge directly calls the Gemini API via `/api/query` (backend).
    *   **Task Request:** The Concierge sends the query to `/api/generate_plan` (backend) to generate a plan.
4.  **Plan Execution (if needed):** The Concierge executes the plan step-by-step, delegating tasks to other assistants via `/api/execute_assistant_task` (backend).
5.  **Backend Processing:** The backend (`app.py`) routes tasks to the appropriate logic (ChromaDB, Gemini API, Internet Search, etc.).
6.  **Response Synthesis:** The backend returns the results to the Concierge.
7.  **Display:** The Concierge displays the final response in the chat window (`App.js`).

---

## Important Notes

*   This is the **FINAL BASE** RAG build. All future projects will be based on this foundation.
*   Ensure the backend and frontend servers are running concurrently.
*   Check the browser console and backend terminal for any error messages.
*   The Concierge Assistant handles both general conversation and task requests.
*   The `sentence-transformers` library is used for relevance scoring (backend).
*   The system uses a local ChromaDB instance (data stored in `./chroma_db`).
*   Uploaded files are stored in the `persistent_documents` directory in the backend.

---

## Troubleshooting

*   **"Failed to fetch" Error:**
    *   Ensure the backend server is running and accessible.
    *   Check the URLs in the frontend code (e.g., `http://localhost:5000/api/generate_plan`) are correct.
    *   Check for CORS-related errors in the browser console.
*   **"Unexpected token '<', "<!doctype "... is not valid JSON" Error:**
    *   This usually means the backend is returning an HTML error page instead of JSON.
    *   Check the backend terminal for any errors.
    *   Verify that the backend routes are correctly defined.
*   **"I couldn't parse the steps from the generated plan" Message:**
    *   This means the plan text returned by the backend is not in the expected format.
    *   Check the format of the plan text in the backend's `/api/generate_plan` endpoint.
    *   Examine the browser console for "Could not parse step line" messages.
*   **ImportError:**
    *   This means a required Python package is not installed.
    *   Ensure you have activated the virtual environment and installed all dependencies using `pip install -r requirements.txt`.

---

## Future Enhancements

*   Implement more sophisticated context selection strategies (e.g., relevance scoring, context compression).
*   Add more robust error handling and logging.
*   Implement a user authentication system.
*   Deploy the application to a production environment (e.g., Heroku, AWS, Azure).
*   Add support for more document types (e.g., web pages, databases).
*   Implement a feedback mechanism to improve the quality of the assistant's responses.